{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define activation functions using TensorFlow library\n",
    "sigmoid_tf = tf.keras.activations.sigmoid\n",
    "relu_tf = tf.keras.activations.relu\n",
    "tanh_tf = tf.keras.activations.tanh\n",
    "softmax_tf = tf.keras.activations.softmax\n",
    "elu_tf = tf.keras.activations.elu\n",
    "gelu_tf = tf.keras.activations.gelu\n",
    "\n",
    "# Example usage\n",
    "x_tf = tf.constant([-1.0, 0.0, 1.0])\n",
    "print(\"Input:\", x_tf.numpy())\n",
    "print(\"Sigmoid:\", sigmoid_tf(x_tf).numpy())\n",
    "print(\"ReLU:\", relu_tf(x_tf).numpy())\n",
    "print(\"Tanh:\", tanh_tf(x_tf).numpy())\n",
    "# print(\"Softmax:\", softmax_tf(x_tf).numpy())\n",
    "print(\"ELU:\", elu_tf(x_tf).numpy())\n",
    "print(\"GELU:\", gelu_tf(x_tf).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Library\n",
    "import numpy as np\n",
    "\n",
    "# Define activation functions without using library\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1])\n",
    "print(\"Input:\", x)\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"Tanh:\", tanh(x))\n",
    "print(\"Softmax:\", softmax(x))\n",
    "print(\"Leaky ReLU:\", leaky_relu(x))\n",
    "print(\"Swish:\", swish(x))\n",
    "print(\"ELU:\", elu(x))\n",
    "print(\"GELU:\", gelu(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing With ANN\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define activation functions\n",
    "activation_functions = ['sigmoid', 'relu', 'tanh', 'softmax', 'elu', 'selu', 'swish', 'gelu']\n",
    "\n",
    "# Function to create model with specified activation function\n",
    "def create_model(activation):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=X_train.shape[1], activation=activation),\n",
    "        Dense(32, activation=activation),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate model with each activation function\n",
    "for activation in activation_functions:\n",
    "    print(f\"\\nTraining model with {activation} activation function...\")\n",
    "    model = create_model(activation)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
