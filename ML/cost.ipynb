{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define sample true values and predicted values\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [2, 2.5, 3.5, 4, 4.5]\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse.numpy())\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae.numpy())\n",
    "\n",
    "# Huber Loss\n",
    "huber_loss = tf.keras.losses.Huber(delta=1.0)(y_true, y_pred)\n",
    "print(\"Huber Loss:\", huber_loss.numpy())\n",
    "\n",
    "# Log Cosh Loss\n",
    "log_cosh_loss = tf.keras.losses.log_cosh(y_true, y_pred)\n",
    "print(\"Log Cosh Loss:\", log_cosh_loss.numpy())\n",
    "\n",
    "# Binary Crossentropy\n",
    "y_true_binary = [0, 1, 1, 0, 1]\n",
    "y_pred_binary = [0.2, 0.8, 0.9, 0.1, 0.7]\n",
    "binary_crossentropy = tf.keras.losses.BinaryCrossentropy()(y_true_binary, y_pred_binary)\n",
    "print(\"Binary Crossentropy:\", binary_crossentropy.numpy())\n",
    "\n",
    "# Categorical Crossentropy\n",
    "y_true_categorical = [[0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0]]\n",
    "y_pred_categorical = [[0.1, 0.8, 0.1], [0.05, 0.05, 0.9], [0.8, 0.1, 0.1], [0.9, 0.05, 0.05], [0.5, 0.3, 0.2]]\n",
    "categorical_crossentropy = tf.keras.losses.CategoricalCrossentropy()(y_true_categorical, y_pred_categorical)\n",
    "print(\"Categorical Crossentropy:\", categorical_crossentropy.numpy())\n",
    "\n",
    "# Sparse Categorical Crossentropy\n",
    "y_true_sparse = [1, 2, 0, 0, 1]\n",
    "y_pred_sparse = [[0.1, 0.8, 0.1], [0.05, 0.05, 0.9], [0.8, 0.1, 0.1], [0.9, 0.05, 0.05], [0.5, 0.3, 0.2]]\n",
    "sparse_categorical_crossentropy = tf.keras.losses.SparseCategoricalCrossentropy()(y_true_sparse, y_pred_sparse)\n",
    "print(\"Sparse Categorical Crossentropy:\", sparse_categorical_crossentropy.numpy())\n",
    "\n",
    "# Hinge Loss\n",
    "y_true_hinge = [1, -1, 1, -1, 1]\n",
    "y_pred_hinge = [0.9, -0.5, 0.8, -0.3, 0.7]\n",
    "hinge_loss = tf.keras.losses.Hinge()(y_true_hinge, y_pred_hinge)\n",
    "print(\"Hinge Loss:\", hinge_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Library\n",
    "import numpy as np\n",
    "# Define functions for loss functions without using library\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return sum((true - pred) ** 2 for true, pred in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return sum(abs(true - pred) for true, pred in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    loss = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if abs(true - pred) <= delta:\n",
    "            loss += 0.5 * (true - pred) ** 2\n",
    "        else:\n",
    "            loss += delta * (abs(true - pred) - 0.5 * delta)\n",
    "    return loss / len(y_true)\n",
    "\n",
    "def log_cosh_loss(y_true, y_pred):\n",
    "    return sum(np.log(np.cosh(pred - true)) for true, pred in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    return -sum(true * np.log(pred) + (1 - true) * np.log(1 - pred) for true, pred in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    return -sum(sum(true * np.log(pred)) for true, pred in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    return -sum(np.log(pred[true]) for true, pred in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    return sum(max(0, 1 - true * pred) for true, pred in zip(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "# Example true and predicted values\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [2, 2.5, 3.5, 4, 4.5]\n",
    "\n",
    "# Compute loss without library\n",
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_true, y_pred))\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_true, y_pred))\n",
    "print(\"Huber Loss:\", huber_loss(y_true, y_pred))\n",
    "print(\"Log Cosh Loss:\", log_cosh_loss(y_true, y_pred))\n",
    "\n",
    "# Binary classification example\n",
    "y_true_binary = [0, 1, 1, 0, 1]\n",
    "y_pred_binary = [0.2, 0.8, 0.9, 0.1, 0.7]\n",
    "print(\"Binary Crossentropy:\", binary_crossentropy(y_true_binary, y_pred_binary))\n",
    "\n",
    "# Multiclass classification example\n",
    "y_true_multiclass = [[0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0]]\n",
    "y_pred_multiclass = [[0.1, 0.8, 0.1], [0.05, 0.05, 0.9], [0.8, 0.1, 0.1], [0.9, 0.05, 0.05], [0.5, 0.3, 0.2]]\n",
    "print(\"Categorical Crossentropy:\", categorical_crossentropy(y_true_multiclass, y_pred_multiclass))\n",
    "\n",
    "# Sparse multiclass classification example\n",
    "y_true_sparse = [1, 2, 0, 0, 1]\n",
    "y_pred_sparse = [[0.1, 0.8, 0.1], [0.05, 0.05, 0.9], [0.8, 0.1, 0.1], [0.9, 0.05, 0.05], [0.5, 0.3, 0.2]]\n",
    "print(\"Sparse Categorical Crossentropy:\", sparse_categorical_crossentropy(y_true_sparse, y_pred_sparse))\n",
    "\n",
    "# Hinge loss example\n",
    "y_true_hinge = [1, -1, 1, -1, 1]\n",
    "y_pred_hinge = [0.9, -0.5, 0.8, -0.3, 0.7]\n",
    "print(\"Hinge Loss:\", hinge_loss(y_true_hinge, y_pred_hinge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing with ANN\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define ANN model\n",
    "def create_ann(loss):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# List of loss functions\n",
    "loss_functions = ['mean_squared_error', 'mean_absolute_error', 'binary_crossentropy',\n",
    "                  'hinge', 'squared_hinge', 'categorical_hinge', 'log_cosh', 'huber']\n",
    "\n",
    "# Train ANN model with each loss function and evaluate on test set\n",
    "for loss_function in loss_functions:\n",
    "    print(\"Training model with\", loss_function, \"loss:\")\n",
    "    model = create_ann(loss_function)\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(\"Test Loss:\", loss)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
