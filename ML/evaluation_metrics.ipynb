{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Class\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == pred:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def precision_score(y_true, y_pred, positive_label=1):\n",
    "    true_positives = sum((true == positive_label) and (pred == positive_label) for true, pred in zip(y_true, y_pred))\n",
    "    false_positives = sum((true != positive_label) and (pred == positive_label) for true, pred in zip(y_true, y_pred))\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def recall_score(y_true, y_pred, positive_label=1):\n",
    "    true_positives = sum((true == positive_label) and (pred == positive_label) for true, pred in zip(y_true, y_pred))\n",
    "    false_negatives = sum((true == positive_label) and (pred != positive_label) for true, pred in zip(y_true, y_pred))\n",
    "    if true_positives + false_negatives == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def f1_score(y_true, y_pred, positive_label=1):\n",
    "    precision = precision_score(y_true, y_pred, positive_label)\n",
    "    recall = recall_score(y_true, y_pred, positive_label)\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Example usage\n",
    "y_true = [1, 0, 1, 1, 0]\n",
    "y_pred = [1, 1, 0, 1, 0]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == pred:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def precision_score(y_true, y_pred, target_class):\n",
    "    true_positives = sum((true == target_class) and (pred == target_class) for true, pred in zip(y_true, y_pred))\n",
    "    false_positives = sum((true != target_class) and (pred == target_class) for true, pred in zip(y_true, y_pred))\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def recall_score(y_true, y_pred, target_class):\n",
    "    true_positives = sum((true == target_class) and (pred == target_class) for true, pred in zip(y_true, y_pred))\n",
    "    false_negatives = sum((true == target_class) and (pred != target_class) for true, pred in zip(y_true, y_pred))\n",
    "    if true_positives + false_negatives == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def f1_score(y_true, y_pred, target_class):\n",
    "    precision = precision_score(y_true, y_pred, target_class)\n",
    "    recall = recall_score(y_true, y_pred, target_class)\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Example usage\n",
    "y_true = [1, 2, 0, 1, 2, 0, 1, 2, 0]\n",
    "y_pred = [1, 2, 0, 1, 1, 0, 1, 2, 2]\n",
    "\n",
    "num_classes = max(y_true) + 1\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for class_label in range(num_classes):\n",
    "    precision.append(precision_score(y_true, y_pred, class_label))\n",
    "    recall.append(recall_score(y_true, y_pred, class_label))\n",
    "    f1.append(f1_score(y_true, y_pred, class_label))\n",
    "    print(f\"Class {class_label} Precision:\", precision[class_label])\n",
    "    print(f\"Class {class_label} Recall:\", recall[class_label])\n",
    "    print(f\"Class {class_label} F1 Score:\", f1[class_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to calculate confusion matrix and AUC-ROC without libraries\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    matrix = [[0] * num_classes for _ in range(num_classes)]\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[true][pred] += 1\n",
    "    return matrix\n",
    "\n",
    "def roc_auc(y_true, y_score, num_classes):\n",
    "    auc = []\n",
    "    for i in range(num_classes):\n",
    "        tp = fp = tn = fn = 0\n",
    "        for true, score in zip(y_true, y_score):\n",
    "            if true == i:\n",
    "                if score >= i:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            else:\n",
    "                if score >= i:\n",
    "                    fp += 1\n",
    "                else:\n",
    "                    tn += 1\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + tn)\n",
    "        auc.append((tpr, fpr))\n",
    "    return auc\n",
    "\n",
    "# Example usage\n",
    "y_true = [1, 2, 0, 1, 2, 0, 1, 2, 0]\n",
    "y_pred = [1, 2, 0, 1, 1, 0, 1, 2, 2]\n",
    "num_classes = max(y_true) + 1\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, num_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "for row in cm:\n",
    "    print(row)\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "y_score = [0.8, 0.6, 0.3, 0.7, 0.9, 0.2, 0.85, 0.65, 0.75]  # Example scores (probabilities)\n",
    "roc_auc_scores = roc_auc(y_true, y_score, num_classes)\n",
    "print(\"AUC-ROC Scores:\")\n",
    "for class_label, (tpr, fpr) in enumerate(roc_auc_scores):\n",
    "    print(f\"Class {class_label} - TPR: {tpr}, FPR: {fpr}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
